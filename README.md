## Data Pipeline with Airflow

### Introduction
For this project I build an ETL pipeline for a database hosted on Redshift, utilizing the automation and monitoring features of Apache Airflow. It was part of my *[Udacity Nanodegree in Data Engineering](https://www.udacity.com/course/data-engineer-nanodegree--nd027)*.

### The Task
A music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and come to the conclusion that the best tool to achieve this is Apache Airflow.

They have decided to bring you into the project and expect you to create high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. They have also noted that the data quality plays a big part when analyses are executed on top the data warehouse and want to run tests against their datasets after the ETL steps have been executed to catch any discrepancies in the datasets.

The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.

### The Dataset
The database is based on two datasets. 
- **The song dataset** `(s3://udacity-dend/song_data)` is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 
- **The logfile dataset** `(s3://udacity-dend/log_data)` consists of log files in JSON format generated by an [Event Simulator](https://github.com/Interana/eventsim) based on the songs in the song dataset. It simulates activity logs from a music streaming app based on specified configurations. The log files are partitioned by year and month.
**The log data JSON path:** `(s3://udacity-dend/log_json_path.json)`

### The Database Schema

The database (db) is modeled after the star schema. [The Star Schema](https://en.wikipedia.org/wiki/Star_schema) separates business process data into facts, which hold the measurable, quantitative data about a business, and dimensions which are descriptive attributes related to fact data. 

For the Sparkify database we have the 'songplays' table as the fact table and the 'songs', 'artists', 'users', and 'time' tables as dimension tables.

### The Project Files

- `dags/`
    - `etl_dag.py` provides all methods to create the dag and connect to airflow
- `plugins/operators/`
    - `stage_redshift.py` custom operator to load data from S3 to the redshift database
    - `load_dimension.py` custom operator to load data from the staging table into the dimension tables
    - `load_fact.py` custom operator to load data from the staging table into the fact table
    - `data_quality.py` custom operator to run data quality checks
- `plugins/helpers/`
    - `create_tables.sql` sql queries to create the database tables 
    - `sql_queries.py` sql queries to populate the db tables 

### The Project was created in the Udacity workspace, here are the steps you need to follow

1. To run the project you need to have a running a Redshift cluster.
2. Run `/opt/airflow/start.sh` in the terminal to start the Airflow server
3. In the Airflow GUI you need to add your credentials in Admin/Connections: 
    - `aws_credentials` to connect to the cluster
    - `redshift` to connect to the database 
4. In the Airflow GUI you can now start the dag
